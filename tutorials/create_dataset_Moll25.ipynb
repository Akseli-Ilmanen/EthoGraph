{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb4bc77",
   "metadata": {},
   "source": [
    "# Create Dataset - Moll 2025 (2 videos)\n",
    "\n",
    "Dataset includes behavioural data from 2 trials:\n",
    "\n",
    "- Videos published in Moll et al., 2025¹, see available online [here](https://www.sciencedirect.com/science/article/pii/S0960982225011005?via%3Dihub#mmc1). We recorded with two cameras (`cam-1`, `cam-2`), but only video from left camera (`cam-2`) is shared.\n",
    "- DeepLabCut² pose files generated for the videos of each camera and 3D pose file `*_DLC_3D.csv` generated using [3D triangulation](https://deeplabcut.github.io/DeepLabCut/docs/Overviewof3D.html). \n",
    "- Video features files (`_s3d.npy`) generated using the Video Features repository³\n",
    "- `Trial_data.nc` file with behavioural features (kinematic, video features), changepoints, custom colours, and trial meta data. \n",
    "\n",
    "Below is a example script how one can generate the `Trial_data.nc` file from the raw data.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "¹ Moll, F. W., Würzler, J., & Nieder, A. (2025). Learned precision tool use in carrion crows. Current Biology, 35(19), 4845-4852.e3. https://doi.org/10.1016/j.cub.2025.08.033\n",
    "\n",
    "² Nath, T., Mathis, A., Chen, A. C., Patel, A., Bethge, M., & Mathis, M. W. (2019). Using DeepLabCut for 3D markerless pose estimation across species and behaviors. Nature Protocols, 14(7), 2152–2176. https://doi.org/10.1038/s41596-019-0176-0\n",
    "\n",
    "³ Iashin, V. (2020). Video Features [Computer software]. https://github.com/v-iashin/video_features\n",
    "\n",
    "<img src=\"assets/moll1.png\" width=\"600\">\n",
    "<img src=\"assets/moll2.png\" width=\"900\">\n",
    "\n",
    "Left: Figure 1C from Moll et al., 2025¹\n",
    "\n",
    "Right: Screenshot from GUI. Bottom line plot shows speed of beak tip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c721ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3dd0b07",
   "metadata": {},
   "source": [
    "### File structure\n",
    "```\n",
    "Moll2025/\n",
    "├── labels/                                    # GUI saved label files\n",
    "├── 2024-12-17_115_Crow1-cam-1.mp4             # video (trial 115)\n",
    "├── 2024-12-17_115_Crow1-cam-1DLC.csv          # 2D pose cam-1\n",
    "├── 2024-12-17_115_Crow1-cam-2DLC.csv          # 2D pose cam-2\n",
    "├── 2024-12-17_115_Crow1_DLC_3D.csv            # 3D triangulated pose\n",
    "├── 2024-12-17_115_Crow1-cam-1_s3d.npy         # Video features\n",
    "├\n",
    "├── 2024-12-18_041_Crow1-cam-1.mp4             # video (trial 41)\n",
    "├── 2024-12-18_041_Crow1-cam-1DLC.csv          # 2D pose cam-1\n",
    "├── 2024-12-18_041_Crow1-cam-2DLC.csv          # 2D pose cam-2\n",
    "├── 2024-12-18_041_Crow1_DLC_3D.csv            # 3D triangulated pose\n",
    "├── 2024-12-18_041_Crow1-cam-1_s3d.npy         # Video features\n",
    "└── Trial_data.nc                              # all behavioural and meta data in one place\n",
    "```\n",
    "\n",
    "### Filename convention\n",
    "\n",
    "```\n",
    "2024-12-17_115_Crow1-cam-1DLC.csv\n",
    "2024-12-17_115_Crow1-cam-1.mp4\n",
    "│            │   │\n",
    "│            │   └── bird ID\n",
    "│            └────── trial number\n",
    "└─────────────────── session date\n",
    "```\n",
    "Using a similar file convention across related files (video, 2D pose, 3D pose), makes it easier to match file names across trials using the  [`set_media_attrs`](../ethograph/utils/io.py) function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zx9r5ektxu8",
   "metadata": {},
   "source": [
    "### Download example data\n",
    "\n",
    "Run the cell below to download the raw data from the GitHub release. If you already have the files locally, this will be skipped automatically. Video features (`*_s3d.npy`) are not included in the release — the cell that loads them will be skipped if missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1v9ujag6qb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethograph import get_project_root\n",
    "from ethograph.utils.download import download_example_dataset\n",
    "\n",
    "data_folder = get_project_root() / \"data\" / \"Moll2025\"\n",
    "download_example_dataset(\"moll2025\", data_folder)\n",
    "\n",
    "print(f\"\\ndata_folder:  {data_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791852c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from movement.io import load_poses\n",
    "from movement.kinematics import compute_pairwise_distances, compute_velocity, compute_acceleration\n",
    "from movement.utils.vector import compute_norm\n",
    "\n",
    "from ethograph.utils.paths import extract_trial_info_from_filename\n",
    "from ethograph import TrialTree, add_angle_rgb_to_ds,  add_changepoints_to_ds, set_media_attrs, get_project_root\n",
    "from ethograph.features.mov_features import compute_distance_to_constant, Position3DCalibration\n",
    "from ethograph.features.changepoints import find_troughs_binary, find_nearest_turning_points_binary\n",
    "from ethograph.features.preprocessing import gaussian_smoothing\n",
    "warnings.filterwarnings(\n",
    "    'ignore', \n",
    "    message='Confidence array was not provided.Setting to an array of NaNs',\n",
    "    module='movement.validators.datasets'\n",
    ")\n",
    "\n",
    "# Config\n",
    "data_folder = get_project_root() / \"data\" / \"Moll2025\"\n",
    "dlc_3d_paths = np.sort(glob.glob(os.path.join(data_folder, \"*_3D.csv\")))\n",
    "\n",
    "fps = 200\n",
    "clip_distance = 50 # Exclude unrealistic distance features (> 50cm)\n",
    "\n",
    "smoothing_params = {\"sigma\": 1.5, \"axis\": 0, \"mode\": \"constant\", \"cval\": np.nan}\n",
    "\n",
    "\n",
    "# We found out that using a subset of s3d video features \n",
    "# (with high Cohen's D for a label) works better than all 1024\n",
    "good_s3d_feats = [ 326, 327, 292, 363, 219, 192, 260, 66, 332, 199,\n",
    "       288, 763, 837, 182, 24, 218, 213, 21, 733, 242] # Crow 1\n",
    "\n",
    "\n",
    "# Stationary locations\n",
    "disp_xyz = [-10.23, -5.907, -1.395]\n",
    "box1 = np.array([-7.08514973,  0.14055037,  0.58930513]) # front left\n",
    "box2 = np.array([-6.97786264,  9.87752058,  0.96104736]) # back left\n",
    "box3 = np.array([6.81007923, 9.72017014, 0.77115876]) # back right\n",
    "box4 = np.array([ 6.77518696, -0.02424044,  0.60184316]) # front right\n",
    "\n",
    "\n",
    "ds_list = []\n",
    "for dlc_3d_path in dlc_3d_paths:\n",
    "    ds = load_poses.from_dlc_file(dlc_3d_path, fps=fps)\n",
    "\n",
    "    # Meta data (available in filename)\n",
    "    session_date, trial_num, bird = extract_trial_info_from_filename(dlc_3d_path)\n",
    "    ds = ds.assign_coords(individuals=[bird])\n",
    "    ds.attrs[\"trial\"] = trial_num \n",
    "    ds.attrs[\"bird\"] = bird    \n",
    "    ds.attrs[\"session_date\"] = session_date \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # dlc_suffix = \"DLC_resnet50_Felix_cross_SessionsAug1shuffle1_200000_filtered\"\n",
    "    \n",
    "    # E.g. mapping trial 41 -> 2024-12-18_041_Crow1-cam-1.mp4\n",
    "    base_name = os.path.basename(dlc_3d_path)\n",
    "    ds = set_media_attrs(\n",
    "        ds,\n",
    "        cameras=[\n",
    "            base_name.replace(\"_DLC_3D.csv\", \"-cam-1.mp4\")\n",
    "            # base_name.replace(\"_DLC_3D.csv\", \"-cam-2.mp4\"),\n",
    "        ],\n",
    "        pose=[\n",
    "            base_name.replace(\"_DLC_3D.csv\", f\"-cam-1DLC.csv\"),\n",
    "            base_name.replace(\"_DLC_3D.csv\", f\"-cam-2DLC.csv\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "    # Transform 2D pose -> 3D pose\n",
    "    calibration = Position3DCalibration()\n",
    "    ds = calibration.transform(ds)\n",
    "    \n",
    "     \n",
    "    # Kinematic features\n",
    "    ds[\"position\"] = gaussian_smoothing(ds.position, **smoothing_params)\n",
    "    ds[\"velocity\"] = compute_velocity(ds.position.sel(keypoints=['stickTip', 'beakTip'])).clip(min=-150, max=150)\n",
    "    ds[\"speed\"] = compute_norm(ds.velocity.sel(keypoints=['stickTip', 'beakTip']))\n",
    "    smooth_2x = {**smoothing_params, \"sigma\": smoothing_params[\"sigma\"] * 2}\n",
    "    position_smooth = gaussian_smoothing(ds.position, **smooth_2x)\n",
    "    ds[\"acceleration\"] = compute_acceleration(position_smooth.sel(keypoints=['stickTip', 'beakTip'])).clip(min=-1500, max=1500)\n",
    "        \n",
    "    \n",
    "\n",
    "    # Distance features\n",
    "    ds[\"pellet_beakTip_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"pellet\": \"beakTip\"}).clip(0, clip_distance)\n",
    "    ds[\"pellet_stickTip_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"pellet\": \"stickTip\"}).clip(0, clip_distance)\n",
    "    ds[\"disp_beakTip_dist\"] = compute_distance_to_constant(ds.position, reference_point=disp_xyz, keypoint=\"beakTip\").clip(0, clip_distance)\n",
    "    ds[\"disp_stickTip_dist\"] = compute_distance_to_constant(ds.position, reference_point=disp_xyz, keypoint=\"stickTip\").clip(0, clip_distance)\n",
    "    \n",
    "    ds[\"sticktip_cornerLFront_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"stickTip\": \"box1\"}).clip(0, clip_distance)\n",
    "    ds[\"sticktip_cornerLBack_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"stickTip\": \"box2\"}).clip(0, clip_distance)\n",
    "    ds[\"sticktip_cornerRBack_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"stickTip\": \"box3\"}).clip(0, clip_distance)\n",
    "    ds[\"sticktip_cornerRFront_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"stickTip\": \"box4\"}).clip(0, clip_distance)\n",
    "\n",
    "    ds[\"beakTip_cornerLFront_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"beakTip\": \"box1\"}).clip(0, clip_distance)\n",
    "    ds[\"beakTip_cornerLBack_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"beakTip\": \"box2\"}).clip(0, clip_distance)\n",
    "    ds[\"beakTip_cornerRBack_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"beakTip\": \"box3\"}).clip(0, clip_distance)\n",
    "    ds[\"beakTip_cornerRFront_dist\"] = compute_pairwise_distances(ds.position, \"keypoints\", {\"beakTip\": \"box4\"}).clip(0, clip_distance)\n",
    "\n",
    "\n",
    "    # Only keep subset of keypoints\n",
    "    ds = ds.sel(keypoints=[\"beakTip\", \"stickTip\", \"pellet\"])\n",
    "\n",
    "    # Video Features (not included in GitHub release download)\n",
    "    s3d_file = base_name.replace(\"_DLC_3D.csv\", \"-cam-1_s3d.npy\")\n",
    "    s3d_path = os.path.join(data_folder, s3d_file)\n",
    "    if os.path.exists(s3d_path):\n",
    "        s3d_data = np.load(s3d_path)\n",
    "        ds[\"s3d\"] = ((\"time\", \"s3d_dims\"), s3d_data[:, good_s3d_feats])\n",
    "    else:\n",
    "        print(f\"Skipping video features (not found): {s3d_file}\")\n",
    "\n",
    "\n",
    "    # Filter for variables shown in Feature DropDown ('Data controls')\n",
    "    for var in ds.data_vars:\n",
    "        ds[var].attrs[\"type\"] = \"features\"\n",
    "        \n",
    "    \n",
    "    # For changepoint correction on speed curve\n",
    "    ds = add_changepoints_to_ds(\n",
    "        ds=ds, \n",
    "        target_feature=\"speed\",\n",
    "        changepoint_name=\"troughs\", \n",
    "        changepoint_func=find_troughs_binary, \n",
    "        prominence=0.5, \n",
    "        distance=2\n",
    "    )\n",
    "    ds = add_changepoints_to_ds(\n",
    "        ds=ds,\n",
    "        target_feature=\"speed\",\n",
    "        changepoint_name=\"turning_points\",\n",
    "        changepoint_func=find_nearest_turning_points_binary,\n",
    "        threshold=1.0,\n",
    "        max_value=50,\n",
    "        prominence=5,\n",
    "        width=2\n",
    "    )\n",
    "    \n",
    "    # Colouring for Lineplots\n",
    "    ds = add_angle_rgb_to_ds(ds, smoothing_params=smoothing_params)\n",
    "    \n",
    "    \n",
    "    # Trial meta data -> Filter trials in 'Navigation controls'\n",
    "    if int(trial_num) == 41:\n",
    "        ds.attrs[\"pellet_position\"] = \"right\"\n",
    "    if int(trial_num) == 115:\n",
    "        ds.attrs[\"pellet_position\"] = \"left\"\n",
    "\n",
    "\n",
    "    # IGNORE, for later use\n",
    "    \n",
    "    # # Pellet position and num pellets is detected from video using custom matlab code (not shared). Saved in 'trial_info.csv'.\n",
    "    # ADD CSV\n",
    "    # if trial_num in trial_info.index:\n",
    "    #     row = trial_info.loc[trial_num]\n",
    "    #     poscat_map = {0: \"no pellet\", 1: \"left\", 2: \"middle\", 3: \"right\"}\n",
    "    #     ds.attrs[\"pellet_position\"] = poscat_map.get(int(row[\"poscat\"]))\n",
    "    #     ds.attrs[\"num_pellets\"] = int(row[\"num_pellets\"])\n",
    "        \n",
    "    ds_list.append(ds)\n",
    "\n",
    "\n",
    "\n",
    "dt = TrialTree.from_datasets(ds_list)\n",
    "dt.to_netcdf(os.path.join(data_folder, \"Trial_data2.nc\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethograph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
