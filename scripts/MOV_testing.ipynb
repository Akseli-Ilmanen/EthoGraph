{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.itrial(20).attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1255d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661f7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate per-frame motion score from video using frame differencing.\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def bitrate_motion(video_path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    cmd = [\n",
    "        \"ffprobe\", \"-select_streams\", \"v\", \"-show_frames\",\n",
    "        \"-show_entries\", \"frame=pts_time,pkt_size\",\n",
    "        \"-of\", \"json\", video_path\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    frames = json.loads(result.stdout)[\"frames\"]\n",
    "    \n",
    "    timestamps = np.array([float(f[\"pts_time\"]) for f in frames])\n",
    "    sizes = np.array([int(f[\"pkt_size\"]) for f in frames])\n",
    "    return timestamps, sizes\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\Admin\\Downloads\\video.mp4\"\n",
    "idx, scores = bitrate_motion(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21b1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef2666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01fea6ba",
   "metadata": {},
   "source": [
    "## Modify AllTrials.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409de5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "from ethograph.utils.io_matlab import update_dt_with_matlab_pulse_onsets\n",
    "from ethograph.features.movement import compute_aux_velocity_and_speed\n",
    "from ethograph.features.preprocessing import resample_to_frames, clip_by_percentiles, downsample_with_antialiasing\n",
    "from ethograph import TrialTree\n",
    "\n",
    "\n",
    "user = \"Alice\"\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "json_file = os.path.join(desktop_path, \"user_paths.json\")\n",
    "with open(json_file, \"r\") as file:\n",
    "    paths = json.load(file)\n",
    "raw_videos_folder = paths[user][\"raw_videos_folder\"]\n",
    "\n",
    "\n",
    "fps = 200\n",
    "\n",
    "# raw!\n",
    "behav_folders = [\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250527_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250527_02\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250528_01\\behav\",            \n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250526_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250526_02\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250528_02\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250529_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250530_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-03_id-Freddy\\ses-000_date-20250602_01\\behav\",\n",
    "                \n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250306_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250307_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250308_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250309_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250503_02\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250504_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250505_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250506_02\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250507_02\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250507_03\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250508_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250508_02\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250509_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250512_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250513_01\\behav\",\n",
    "                \n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250514_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250515_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250516_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250519_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250521_01\\behav\",\n",
    "                r\"D:\\Alice\\AK_data\\rawdata\\sub-01_id-Ivy\\ses-000_date-20250522_01\\behav\"       \n",
    "                ]\n",
    "\n",
    "accel_path = r\"D:\\Alice\\AK_data\\rawdata\\accelerometer_copied\"\n",
    "\n",
    "def read_intan_auxiliary(filepath: str, n_channels: int = 3) -> np.ndarray:\n",
    "    \"\"\"Read Intan auxiliary.dat file. Returns array in volts.\"\"\"\n",
    "    raw = np.fromfile(filepath, dtype=np.uint16)\n",
    "    return raw.reshape(-1, n_channels) * 0.0000374\n",
    "\n",
    "def read_intan_timestamps(filepath: str, sample_rate: float = 30000.0) -> np.ndarray:\n",
    "    raw = np.fromfile(filepath, dtype=np.int32)\n",
    "    return raw / sample_rate\n",
    "\n",
    "for behav_folder in behav_folders:\n",
    "    print(f\"Processing folder: {behav_folder}\")\n",
    "    \n",
    "    deriv_behav_folder = behav_folder.replace(\"rawdata\", \"derivatives\")\n",
    "    nc_path = Path(deriv_behav_folder) / \"Trial_data.nc\"\n",
    "    \n",
    "    dt = TrialTree.open(nc_path)\n",
    "    \n",
    "    \n",
    "\n",
    "    AllTrialsPath = os.path.join(deriv_behav_folder, \"Trial_data.mat\")\n",
    "    AllTrialsPath = AllTrialsPath.replace(r\"D:\\Alice\\AK_data\", r\"E:\\AK_data\") # AllTrils.mat where pulse onsets are (after merge of left and right PC)\n",
    "\n",
    "    # dt = update_dt_with_matlab_pulse_onsets(nc_path, AllTrialsPath)\n",
    "    dt_accel = dt.copy()  \n",
    "    def keep_pulse_onsets(ds):\n",
    "        keep = {k: ds[k] for k in ds.data_vars if k == \"pulse_onsets\"}\n",
    "        return xr.Dataset(keep, attrs=ds.attrs)\n",
    "    dt_accel = dt_accel.map_over_datasets(keep_pulse_onsets)\n",
    "    dt_accel = TrialTree(dt_accel)\n",
    "        \n",
    "\n",
    "    \n",
    "    session = dt.itrial(0).session\n",
    "    session = session.replace(\"-\", \"\")\n",
    "    bird = dt.itrial(0).bird\n",
    "    \n",
    "    dat_path = Path(accel_path) / bird / session / \"auxiliary.dat\"\n",
    "    time_path = Path(accel_path) / bird / session / \"time.dat\"\n",
    "    \n",
    "    aux = read_intan_auxiliary(dat_path)\n",
    "    time = read_intan_timestamps(time_path)\n",
    "    \n",
    "    for trial in dt.trials:\n",
    "        time_frames = dt.trial(trial).time.values\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pulse_onsets = dt.trial(trial).pulse_onsets.values\n",
    "    \n",
    "\n",
    "    \n",
    "        trial_sample_indices = np.arange(pulse_onsets[0], pulse_onsets[-1] + 150)\n",
    "        \n",
    "        aux_trial = aux[trial_sample_indices, :]\n",
    "        time_trial = time[trial_sample_indices] - time[trial_sample_indices[0]]\n",
    "\n",
    "        # aux_accel, aux_velocity, aux_speed = compute_aux_velocity_and_speed(\n",
    "        #     aux_trial, time_trial, \n",
    "        #     mov_mean_window1=6001, \n",
    "        #     mov_mean_window2=15001\n",
    "        # )\n",
    "\n",
    "        # ds = dt[f\"trial_{trial}\"].to_dataset().copy()        \n",
    "\n",
    "\n",
    "        # # As model feature\n",
    "        # time_frames = dt.trial(trial).time.values\n",
    "        # aux_acceleration = resample_to_frames(aux_accel, time_trial, time_frames)\n",
    "        # aux_acceleration = clip_by_percentiles(aux_acceleration, (5, 95))\n",
    "        # aux_velocity = resample_to_frames(aux_velocity, time_trial, time_frames)\n",
    "        # aux_speed = resample_to_frames(aux_speed, time_trial, time_frames)        \n",
    "        \n",
    "        # ds['aux_acceleration'] = xr.DataArray(\n",
    "        #     aux_acceleration,\n",
    "        #     dims=['time', 'space'],\n",
    "        #     coords={'time': time_frames, 'space': ['x', 'y', 'z']}\n",
    "        # )\n",
    "        # ds['aux_velocity'] = xr.DataArray(\n",
    "        #     aux_velocity,\n",
    "        #     dims=['time', 'space'],\n",
    "        #     coords={'time': time_frames, 'space': ['x', 'y', 'z']}\n",
    "        # )\n",
    "        # ds['aux_speed'] = xr.DataArray(\n",
    "        #     aux_speed,\n",
    "        #     dims=['time'],\n",
    "        #     coords={'time': time_frames}\n",
    "        # )\n",
    "        # for feat in [\"aux_acceleration\", \"aux_velocity\", \"aux_speed\"]:\n",
    "        #     ds[feat].attrs[\"type\"] = \"features\"    \n",
    "        \n",
    "        # dt[f\"trial_{trial}\"] = ds\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    downsample_factor = 10  # 30000 -> 3000 Hz\n",
    "    accel_downsampled = downsample_with_antialiasing(aux_trial, downsample_factor)\n",
    " \n",
    "    time_downsampled = time_trial[::downsample_factor][:len(accel_downsampled)]\n",
    "\n",
    "    for trial in dt_accel:\n",
    "        ds = dt[trial].to_dataset().copy()   \n",
    "        \n",
    "        ds['acceleration'] = xr.DataArray(\n",
    "            accel_downsampled,\n",
    "            dims=['time_1000Hz', 'space'],\n",
    "            coords={'time_1000Hz': time_downsampled, 'space': ['x', 'y', 'z']},\n",
    "            attrs={'type': 'features'},\n",
    "        )\n",
    "        dt_accel[trial] = ds           \n",
    "    \n",
    "    # dt.save()\n",
    "    dt_accel.save(nc_path.with_stem(\"Trial_data_accelerometer\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethograph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
